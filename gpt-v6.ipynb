{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6679ea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Rodando no dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677e5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIPERPARÂMETROS v6 \n",
    "\n",
    "# O Contexto (Memória de curto prazo)\n",
    "# Mantemos 256 para economizar VRAM e investir em \"Inteligência\" (n_embd)\n",
    "block_size = 256      \n",
    "\n",
    "# O Tamanho do Lote\n",
    "# Reduzimos de 32 para 16 porque o modelo ficou \"gordo\" e profundo.\n",
    "# Se deixar 32 com a config abaixo, vai dar \"Out of Memory\".\n",
    "batch_size = 16       \n",
    "\n",
    "# Duração do Treino\n",
    "# Aumentamos para dar tempo de ler o BrWaC\n",
    "max_iters = 10000     \n",
    "\n",
    "# Taxa de Aprendizado\n",
    "learning_rate = 3e-4  \n",
    "\n",
    "# Frequência de Avaliação\n",
    "eval_iters = 200      \n",
    "\n",
    "# A Largura (Inteligência/Vocabulário)\n",
    "# Aumentamos de 256 para 384. \n",
    "n_embd = 384          \n",
    "\n",
    "# A Profundidade (Raciocínio)\n",
    "n_layer = 8           \n",
    "\n",
    "# Cabeças de Atenção\n",
    "# 384 dividido por 6 = 64\n",
    "n_head = 6            \n",
    "\n",
    "# Dropout\n",
    "# Reduzido para 0.1 pois agora temos muitos dados (BrWaC), \n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72007fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pega TODOS os arquivos .txt da pasta data/\n",
    "files = glob.glob('data/*.txt')\n",
    "print(f\"Encontrados {len(files)} arquivos para treinamento.\")\n",
    "\n",
    "# Palavras para banir\n",
    "blacklist = [\n",
    "    \"Página\", \"Page\", \n",
    "    \"Colleen Hoover\", \"Machado de Assis\", \n",
    "    \"Sumário\", \"Capítulo\", \"Copyright\", \n",
    "    \"Todos os direitos reservados\"\n",
    "]\n",
    "\n",
    "all_text_content = \"\" # Vamos acumular tudo numa string gigante\n",
    "\n",
    "for file_name in files:\n",
    "    try:\n",
    "        with open(file_name, 'r', encoding='utf-8') as f:\n",
    "            raw_text = f.read()\n",
    "            \n",
    "            # FASE DE LIMPEZA\n",
    "            clean_lines = []\n",
    "            lines = raw_text.split('\\n')\n",
    "            \n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                \n",
    "                # Filtros de Lixo (Números de página, linhas vazias)\n",
    "                if not line: continue\n",
    "                if line.isdigit(): continue # Remove \"12\", \"45\"\n",
    "                \n",
    "                # Filtro de Blacklist (Cabeçalhos repetidos)\n",
    "                if any(bad_word in line for bad_word in blacklist):\n",
    "                    continue\n",
    "                \n",
    "                # Filtro de tamanho (Evita linhas com 1 letra que não sejam pontuação)\n",
    "                if len(line) < 2 and line not in ['.', '?', '!', '—']:\n",
    "                    continue\n",
    "\n",
    "                clean_lines.append(line)\n",
    "            \n",
    "            # Juntar as linhas limpas deste livro\n",
    "            book_text = \"\\n\".join(clean_lines)\n",
    "            \n",
    "            # Adicionar ao texto total com o marcador especial\n",
    "            # O marcador <|endoftext|> avisa o modelo que a história acabou\n",
    "            all_text_content += book_text + \" <|endoftext|> \\n\"\n",
    "            \n",
    "            print(f\" - Processado: {file_name} ({len(clean_lines)} linhas úteis)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler {file_name}: {e}\")\n",
    "\n",
    "text = all_text_content\n",
    "print(f\"Dataset FINAL criado com {len(text)/1e6:.2f} MB de texto.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f785a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZAÇÃO TIKTOKEN\n",
    "print(\"Tokenizando com GPT-2 BPE\")\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "encoded_data = enc.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "vocab_size = enc.n_vocab # Isso será 50257 (Padrão GPT-2)\n",
    "print(f\"Tamanho do Vocabulário: {vocab_size} tokens únicos\")\n",
    "print(f\"Total de Tokens para treino: {len(encoded_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca93cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Em vez de pegar os primeiros 90% (que é só Machado) e testar nos 10% (que é só SAC),\n",
    "# vamos criar pedaços aleatórios.\n",
    "data = torch.tensor(encoded_data, dtype=torch.long)\n",
    "\n",
    "# Dividir em blocos aleatórios para garantir mistura de domínios\n",
    "# Mas como estamos num modelo simples de sequencia, o jeito mais fácil e seguro\n",
    "# para manter a coerência do texto é garantir que o split seja feito DEPOIS de misturar \n",
    "# se tivéssemos amostras independentes.\n",
    "# Como é um texto contínuo, vamos fazer o seguinte:\n",
    "\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b69c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infraestrutura do Modelo Transformer\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_metrics():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        accuracies = torch.zeros(eval_iters)\n",
    "\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            pred = torch.argmax(probs, dim=-1)\n",
    "            acc = (pred == Y.view(-1)).float().mean()\n",
    "            accuracies[k] = acc.item()\n",
    "\n",
    "        out[split] = {\n",
    "            'loss': losses.mean(),\n",
    "            'acc': accuracies.mean()\n",
    "        }\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   \n",
    "        q = self.query(x) \n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5 \n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) \n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v = self.value(x)\n",
    "        out = wei @ v \n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "        \n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Anteriormente no gpt-v3 o vocab_size era 95 vocab_size agora é 50257\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        tok_emb = self.token_embedding_table(index) \n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) \n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x) \n",
    "        x = self.ln_f(x) \n",
    "        logits = self.lm_head(x) \n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        temperature: 1.0 = normal. >1.0 = mais criativo/doido. <1.0 = mais focado/conservador.\n",
    "        top_k: Se definido (ex: 50), só escolhe entre as 50 melhores palavras. Evita erros bizarros.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Corta o contexto se passar do block_size\n",
    "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
    "            \n",
    "            # Pega as predições\n",
    "            logits, _ = self(idx_cond)\n",
    "            \n",
    "            # Foca apenas no último passo de tempo e aplica temperatura\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            \n",
    "            # Opcional: Corta as opções ruins (Top-K Sampling)\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "                \n",
    "            # Calcula probabilidades\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sorteia o próximo token\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd41ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execução e Checkpointing\n",
    "def save_checkpoint(step, model, optimizer, loss_train, loss_val, acc_train, acc_val, filename=\"checkpoint_gpt_v6.pth\"):\n",
    "    checkpoint = {\n",
    "        'step': step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss_train': loss_train,\n",
    "        'loss_val': loss_val,\n",
    "        'acc_train': acc_train,\n",
    "        'acc_val': acc_val\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"Checkpoint salvo em step {step}\")\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename=\"checkpoint_gpt_v6.pth\"):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"Carregando checkpoint '{filename}'...\")\n",
    "        checkpoint = torch.load(filename, map_location=device)\n",
    "\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "        step = checkpoint['step']\n",
    "        loss_train = checkpoint.get('loss_train', [])\n",
    "        loss_val = checkpoint.get('loss_val', [])\n",
    "\n",
    "        acc_train = checkpoint.get('acc_train', []) \n",
    "        acc_val = checkpoint.get('acc_val', [])\n",
    "\n",
    "        print(f\"Retomando do step {step}\")\n",
    "        return step, loss_train, loss_val, acc_train, acc_val\n",
    "    return 0, [], [], [], []\n",
    "\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "print(f\"Modelo PT-BR criado com {sum(p.numel() for p in m.parameters())/1e6:.2f} M parâmetros\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Decaimento cosseno\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iters, eta_min=1e-5)\n",
    "\n",
    "# Tenta carregar checkpoint\n",
    "start_iter, loss_history_train, loss_history_val, acc_history_train, acc_history_val = load_checkpoint(model, optimizer)\n",
    "\n",
    "print(\"Iniciando treinamento em Português...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for iter in range(start_iter, max_iters):\n",
    "    \n",
    "    if iter % eval_iters == 0:\n",
    "        metrics = estimate_metrics()\n",
    "\n",
    "        t_loss, v_loss = metrics['train']['loss'], metrics['val']['loss']\n",
    "        t_acc, v_acc = metrics['train']['acc'], metrics['val']['acc']\n",
    "\n",
    "        print(f\"step {iter}: loss {t_loss:.3f}/{v_loss:.3f} | acc {t_acc:.3f}/{v_acc:.3f}\")\n",
    "\n",
    "        # Atualiza históricos\n",
    "        loss_history_train.append(t_loss)\n",
    "        loss_history_val.append(v_loss)\n",
    "        acc_history_train.append(t_acc)\n",
    "        acc_history_val.append(v_acc)\n",
    "        \n",
    "        # Salvar checkpoint\n",
    "        save_checkpoint(iter, model, optimizer, \n",
    "                        loss_history_train, loss_history_val, \n",
    "                        acc_history_train, acc_history_val)\n",
    "\n",
    "    # Backpropagation padrão\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Treinamento finalizado em {(end_time - start_time)/60:.2f} minutos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ec2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotagem do Gráfico\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Gráfico 1: Loss\n",
    "ax1.plot(loss_history_train, label='Treino')\n",
    "ax1.plot(loss_history_val, label='Validação')\n",
    "ax1.set_title('Evolução da Loss (Entropia Cruzada)')\n",
    "ax1.set_xlabel(f'Iterações (x {eval_iters})')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Gráfico 2: Accuracy\n",
    "ax2.plot(acc_history_train, label='Treino')\n",
    "ax2.plot(acc_history_val, label='Validação')\n",
    "ax2.set_title('Evolução da Accuracy (Precisão de Tokens)')\n",
    "ax2.set_xlabel(f'Iterações (x {eval_iters})')\n",
    "ax2.set_ylabel('Accuracy (0.0 a 1.0)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grafico_completo_metrics.png')\n",
    "print(\"Gráfico salvo: grafico_completo_metrics.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8d4376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geração de Texto\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"GERANDO TEXTO EM PORTUGUÊS\")\n",
    "print(\"=\"*30 + \"\\n\")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d258ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar texto a partir do vazio\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "generated_ids = model.generate(context, max_new_tokens=200, temperature=0.8, top_k=50)\n",
    "\n",
    "# Decode usando tiktoken\n",
    "print(enc.decode(generated_ids[0].tolist()))\n",
    "\n",
    "# Salvando\n",
    "torch.save(model.state_dict(), 'gpt_ptbr_v6.pth')\n",
    "print(\"Modelo salvo como gpt_ptbr_v6.pth\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
