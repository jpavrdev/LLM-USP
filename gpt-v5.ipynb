{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "28475b84",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch.nn import functional as F\n",
                "import tiktoken\n",
                "import matplotlib.pyplot as plt\n",
                "import time\n",
                "import os\n",
                "import glob"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "id": "7e607b44",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Nessa versão estamos aplicando uma técnica chamada 'Data Interleaving'\n",
                "# Estaremos intercalando os dados\n",
                "# Em vez de pegar os primeiros 90% (que é só Machado) e testar nos 10% (que é só SAC)\n",
                "# vamos criar pedaços aleatórios.\n",
                "# Dividir em blocos aleatórios para garantir mistura de domínios\n",
                "# Mas como estamos num modelo simples de sequencia, o jeito mais fácil e seguro\n",
                "# para manter a coerência do texto é garantir que o split seja feito DEPOIS de misturar \n",
                "# O texto gerado será um Frankenstein, entretanto agora ele irá aprender a gramática correta dos textos\n",
                "# Além disso aumentamos as configurações, pois estamos usando uma RX 5600 (6GB VRAM) e 16GB de RAM DDR4 3200mhz para os testes\n",
                "# Então aumentamos o poder de processamento da nossa LLM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "id": "f3ec0c93",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Rodando no dispositivo: cpu\n"
                    ]
                }
            ],
            "source": [
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Rodando no dispositivo: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "1f7b63d7",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Otimizadas para RX 5600 (6GB VRAM)\n",
                "block_size = 256      # AUMENTO: De 64 para 256. (Lê parágrafos inteiros agora)\n",
                "batch_size = 32       # AUMENTO: De 16 para 32.\n",
                "max_iters = 5000      # Mantém\n",
                "learning_rate = 3e-4  # Mantém\n",
                "eval_iters = 200\n",
                "n_embd = 256          # De 128 para 256. (Vetores mais ricos para o vocabulário BPE)\n",
                "n_layer = 6           # AUMENTO: De 4 para 6. (Mais profundidade de raciocínio)\n",
                "n_head = 8            # AUMENTO: De 4 para 8. (Tem que ser divisor de n_embd: 256 / 8 = 32)\n",
                "dropout = 0.2         # Mantém para evitar overfitting"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "0ad37670",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Encontrados 13 arquivos para treinamento.\n",
                        " - Processado: data/constituicao_federal.txt (3575 linhas úteis)\n",
                        " - Processado: data/verity.txt (3258 linhas úteis)\n",
                        " - Processado: data/uma_segunda_chance.txt (5064 linhas úteis)\n",
                        " - Processado: data/se_nao_fosse_voce.txt (5781 linhas úteis)\n",
                        " - Processado: data/um_caso_perdido.txt (3299 linhas úteis)\n",
                        " - Processado: data/lado_feio_do_amor.txt (4490 linhas úteis)\n",
                        " - Processado: data/assim_que_acaba.txt (4626 linhas úteis)\n",
                        " - Processado: data/dom_casmurro.txt (6804 linhas úteis)\n",
                        " - Processado: data/assim_que_comeca.txt (4650 linhas úteis)\n",
                        " - Processado: data/talvez_nao_colleen_hoover.txt (1321 linhas úteis)\n",
                        " - Processado: data/memorias_postumas.txt (6475 linhas úteis)\n",
                        " - Processado: data/todas_as_suas_imperfeicoes.txt (3486 linhas úteis)\n",
                        " - Processado: data/sem_esperancas.txt (8156 linhas úteis)\n",
                        "Dataset FINAL criado com 6.20 MB de texto.\n"
                    ]
                }
            ],
            "source": [
                "# Pega TODOS os arquivos .txt da pasta data/\n",
                "files = glob.glob('data/*.txt')\n",
                "print(f\"Encontrados {len(files)} arquivos para treinamento.\")\n",
                "\n",
                "# Palavras para banir\n",
                "blacklist = [\n",
                "    \"Página\", \"Page\", \n",
                "    \"Colleen Hoover\", \"Machado de Assis\", \n",
                "    \"Sumário\", \"Capítulo\", \"Copyright\", \n",
                "    \"Todos os direitos reservados\"\n",
                "]\n",
                "\n",
                "all_text_content = \"\" # Vamos acumular tudo numa string gigante\n",
                "\n",
                "for file_name in files:\n",
                "    try:\n",
                "        with open(file_name, 'r', encoding='utf-8') as f:\n",
                "            raw_text = f.read()\n",
                "            \n",
                "            # FASE DE LIMPEZA\n",
                "            clean_lines = []\n",
                "            lines = raw_text.split('\\n')\n",
                "            \n",
                "            for line in lines:\n",
                "                line = line.strip()\n",
                "                \n",
                "                # Filtros de Lixo (Números de página, linhas vazias)\n",
                "                if not line: continue\n",
                "                if line.isdigit(): continue # Remove \"12\", \"45\"\n",
                "                \n",
                "                # Filtro de Blacklist (Cabeçalhos repetidos)\n",
                "                if any(bad_word in line for bad_word in blacklist):\n",
                "                    continue\n",
                "                \n",
                "                # Filtro de tamanho (Evita linhas com 1 letra que não sejam pontuação)\n",
                "                if len(line) < 2 and line not in ['.', '?', '!', '—']:\n",
                "                    continue\n",
                "\n",
                "                clean_lines.append(line)\n",
                "            \n",
                "            # Juntar as linhas limpas deste livro\n",
                "            book_text = \"\\n\".join(clean_lines)\n",
                "            \n",
                "            # Adicionar ao texto total com o marcador especial\n",
                "            # O marcador <|endoftext|> avisa o modelo que a história acabou\n",
                "            all_text_content += book_text + \" <|endoftext|> \\n\"\n",
                "            \n",
                "            print(f\" - Processado: {file_name} ({len(clean_lines)} linhas úteis)\")\n",
                "\n",
                "    except Exception as e:\n",
                "        print(f\"Erro ao ler {file_name}: {e}\")\n",
                "\n",
                "text = all_text_content\n",
                "print(f\"Dataset FINAL criado com {len(text)/1e6:.2f} MB de texto.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "238b46c3",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Tokenizando com GPT-2 BPE\n",
                        "Tamanho do Vocabulário: 50257 tokens únicos\n",
                        "Total de Tokens para treino: 2506590\n"
                    ]
                }
            ],
            "source": [
                "# TOKENIZAÇÃO TIKTOKEN\n",
                "print(\"Tokenizando com GPT-2 BPE\")\n",
                "enc = tiktoken.get_encoding(\"gpt2\")\n",
                "encoded_data = enc.encode(text, allowed_special={\"<|endoftext|>\"})\n",
                "vocab_size = enc.n_vocab # Isso será 50257 (Padrão GPT-2)\n",
                "print(f\"Tamanho do Vocabulário: {vocab_size} tokens únicos\")\n",
                "print(f\"Total de Tokens para treino: {len(encoded_data)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "f00db481",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Em vez de pegar os primeiros 90% (que é só Machado) e testar nos 10% (que é só SAC),\n",
                "# vamos criar pedaços aleatórios.\n",
                "data = torch.tensor(encoded_data, dtype=torch.long)\n",
                "\n",
                "# Dividir em blocos aleatórios para garantir mistura de domínios\n",
                "# Mas como estamos num modelo simples de sequencia, o jeito mais fácil e seguro\n",
                "# para manter a coerência do texto é garantir que o split seja feito DEPOIS de misturar \n",
                "# se tivéssemos amostras independentes.\n",
                "# Como é um texto contínuo, vamos fazer o seguinte:\n",
                "\n",
                "n = int(0.9 * len(data))\n",
                "train_data = data[:n]\n",
                "val_data = data[n:]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "82501cdc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Infraestrutura do Modelo Transformer\n",
                "def get_batch(split):\n",
                "    data = train_data if split == 'train' else val_data\n",
                "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
                "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
                "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
                "    return x.to(device), y.to(device)\n",
                "\n",
                "@torch.no_grad()\n",
                "def estimate_metrics():\n",
                "    out = {}\n",
                "    model.eval()\n",
                "    for split in ['train', 'val']:\n",
                "        losses = torch.zeros(eval_iters)\n",
                "        accuracies = torch.zeros(eval_iters)\n",
                "\n",
                "        for k in range(eval_iters):\n",
                "            X, Y = get_batch(split)\n",
                "            logits, loss = model(X, Y)\n",
                "            losses[k] = loss.item()\n",
                "\n",
                "            probs = F.softmax(logits, dim=-1)\n",
                "            pred = torch.argmax(probs, dim=-1)\n",
                "            acc = (pred == Y.view(-1)).float().mean()\n",
                "            accuracies[k] = acc.item()\n",
                "\n",
                "        out[split] = {\n",
                "            'loss': losses.mean(),\n",
                "            'acc': accuracies.mean()\n",
                "        }\n",
                "    model.train()\n",
                "    return out\n",
                "\n",
                "class Head(nn.Module):\n",
                "    def __init__(self, head_size):\n",
                "        super().__init__()\n",
                "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
                "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
                "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
                "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x):\n",
                "        B, T, C = x.shape\n",
                "        k = self.key(x)   \n",
                "        q = self.query(x) \n",
                "        wei = q @ k.transpose(-2, -1) * C**-0.5 \n",
                "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) \n",
                "        wei = F.softmax(wei, dim=-1)\n",
                "        wei = self.dropout(wei)\n",
                "        v = self.value(x)\n",
                "        out = wei @ v \n",
                "        return out\n",
                "\n",
                "class MultiHeadAttention(nn.Module):\n",
                "    def __init__(self, num_heads, head_size):\n",
                "        super().__init__()\n",
                "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
                "        self.proj = nn.Linear(n_embd, n_embd)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, x):\n",
                "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
                "        out = self.dropout(self.proj(out))\n",
                "        return out\n",
                "\n",
                "class FeedForward(nn.Module):\n",
                "    def __init__(self, n_embd):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Linear(n_embd, 4 * n_embd),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(4 * n_embd, n_embd),\n",
                "            nn.Dropout(dropout),\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x)\n",
                "\n",
                "class Block(nn.Module):\n",
                "    def __init__(self, n_embd, n_head):\n",
                "        super().__init__()\n",
                "        head_size = n_embd // n_head\n",
                "        self.sa = MultiHeadAttention(n_head, head_size)\n",
                "        self.ffwd = FeedForward(n_embd)\n",
                "        self.ln1 = nn.LayerNorm(n_embd)\n",
                "        self.ln2 = nn.LayerNorm(n_embd)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        x = x + self.sa(self.ln1(x))\n",
                "        x = x + self.ffwd(self.ln2(x))\n",
                "        return x\n",
                "        \n",
                "class GPTLanguageModel(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        # Anteriormente no gpt-v3 o vocab_size era 95 vocab_size agora é 50257\n",
                "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
                "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
                "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
                "        self.ln_f = nn.LayerNorm(n_embd)\n",
                "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
                "        self.apply(self._init_weights)\n",
                "\n",
                "    def _init_weights(self, module):\n",
                "        if isinstance(module, nn.Linear):\n",
                "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
                "            if module.bias is not None:\n",
                "                torch.nn.init.zeros_(module.bias)\n",
                "        elif isinstance(module, nn.Embedding):\n",
                "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
                "\n",
                "    def forward(self, index, targets=None):\n",
                "        B, T = index.shape\n",
                "        tok_emb = self.token_embedding_table(index) \n",
                "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) \n",
                "        x = tok_emb + pos_emb\n",
                "        x = self.blocks(x) \n",
                "        x = self.ln_f(x) \n",
                "        logits = self.lm_head(x) \n",
                "\n",
                "        if targets is None:\n",
                "            loss = None\n",
                "        else:\n",
                "            B, T, C = logits.shape\n",
                "            logits = logits.view(B*T, C)\n",
                "            targets = targets.view(B*T)\n",
                "            loss = F.cross_entropy(logits, targets)\n",
                "        return logits, loss\n",
                "\n",
                "    def generate(self, idx, max_new_tokens):\n",
                "        for _ in range(max_new_tokens):\n",
                "            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]\n",
                "            logits, _ = self(idx_cond)\n",
                "            logits = logits[:, -1, :] \n",
                "            probs = F.softmax(logits, dim=-1) \n",
                "            idx_next = torch.multinomial(probs, num_samples=1) \n",
                "            idx = torch.cat((idx, idx_next), dim=1) \n",
                "        return idx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "02f9b7a1",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Modelo PT-BR criado com 30.58 M parâmetros\n",
                        "Iniciando treinamento em Português...\n"
                    ]
                }
            ],
            "source": [
                "# Execução e Checkpointing\n",
                "def save_checkpoint(step, model, optimizer, loss_train, loss_val, acc_train, acc_val, filename=\"checkpoint_gpt_v5.pth\"):\n",
                "    checkpoint = {\n",
                "        'step': step,\n",
                "        'model_state_dict': model.state_dict(),\n",
                "        'optimizer_state_dict': optimizer.state_dict(),\n",
                "        'loss_train': loss_train,\n",
                "        'loss_val': loss_val,\n",
                "        'acc_train': acc_train,\n",
                "        'acc_val': acc_val\n",
                "    }\n",
                "    torch.save(checkpoint, filename)\n",
                "    print(f\"Checkpoint salvo em step {step}\")\n",
                "\n",
                "def load_checkpoint(model, optimizer, filename=\"checkpoint_gpt_v5.pth\"):\n",
                "    if os.path.exists(filename):\n",
                "        print(f\"Carregando checkpoint '{filename}'...\")\n",
                "        checkpoint = torch.load(filename, map_location=device)\n",
                "\n",
                "        model.load_state_dict(checkpoint['model_state_dict'])\n",
                "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
                "\n",
                "        step = checkpoint['step']\n",
                "        loss_train = checkpoint.get('loss_train', [])\n",
                "        loss_val = checkpoint.get('loss_val', [])\n",
                "\n",
                "        acc_train = checkpoint.get('acc_train', []) \n",
                "        acc_val = checkpoint.get('acc_val', [])\n",
                "\n",
                "        print(f\"Retomando do step {step}\")\n",
                "        return step, loss_train, loss_val, acc_train, acc_val\n",
                "    return 0, [], [], [], []\n",
                "\n",
                "model = GPTLanguageModel()\n",
                "m = model.to(device)\n",
                "print(f\"Modelo PT-BR criado com {sum(p.numel() for p in m.parameters())/1e6:.2f} M parâmetros\")\n",
                "\n",
                "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
                "\n",
                "# Tenta carregar checkpoint\n",
                "start_iter, loss_history_train, loss_history_val, acc_history_train, acc_history_val = load_checkpoint(model, optimizer)\n",
                "\n",
                "print(\"Iniciando treinamento em Português...\")\n",
                "start_time = time.time()\n",
                "\n",
                "for iter in range(start_iter, max_iters):\n",
                "    \n",
                "    if iter % eval_iters == 0:\n",
                "        metrics = estimate_metrics()\n",
                "\n",
                "        t_loss, v_loss = metrics['train']['loss'], metrics['val']['loss']\n",
                "        t_acc, v_acc = metrics['train']['acc'], metrics['val']['acc']\n",
                "\n",
                "        print(f\"step {iter}: loss {t_loss:.3f}/{v_loss:.3f} | acc {t_acc:.3f}/{v_acc:.3f}\")\n",
                "\n",
                "        # Atualiza históricos\n",
                "        loss_history_train.append(t_loss)\n",
                "        loss_history_val.append(v_loss)\n",
                "        acc_history_train.append(t_acc)\n",
                "        acc_history_val.append(v_acc)\n",
                "        \n",
                "        # Salvar checkpoint\n",
                "        save_checkpoint(iter, model, optimizer, \n",
                "                        loss_history_train, loss_history_val, \n",
                "                        acc_history_train, acc_history_val)\n",
                "\n",
                "    # Backpropagation padrão\n",
                "    xb, yb = get_batch('train')\n",
                "    logits, loss = model(xb, yb)\n",
                "    optimizer.zero_grad(set_to_none=True)\n",
                "    loss.backward()\n",
                "    optimizer.step()\n",
                "\n",
                "end_time = time.time()\n",
                "print(f\"Treinamento finalizado em {(end_time - start_time)/60:.2f} minutos.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "945b6f0c",
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'plt' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Plotagem do Gráfico\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m      3\u001b[39m plt.plot(loss_history_train, label=\u001b[33m'\u001b[39m\u001b[33mTreino\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m plt.plot(loss_history_val, label=\u001b[33m'\u001b[39m\u001b[33mValidação\u001b[39m\u001b[33m'\u001b[39m)\n",
                        "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
                    ]
                }
            ],
            "source": [
                "# Plotagem do Gráfico\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "# Gráfico 1: Loss\n",
                "ax1.plot(loss_history_train, label='Treino')\n",
                "ax1.plot(loss_history_val, label='Validação')\n",
                "ax1.set_title('Evolução da Loss (Entropia Cruzada)')\n",
                "ax1.set_xlabel(f'Iterações (x {eval_iters})')\n",
                "ax1.set_ylabel('Loss')\n",
                "ax1.legend()\n",
                "ax1.grid(True)\n",
                "\n",
                "# Gráfico 2: Accuracy\n",
                "ax2.plot(acc_history_train, label='Treino')\n",
                "ax2.plot(acc_history_val, label='Validação')\n",
                "ax2.set_title('Evolução da Accuracy (Precisão de Tokens)')\n",
                "ax2.set_xlabel(f'Iterações (x {eval_iters})')\n",
                "ax2.set_ylabel('Accuracy (0.0 a 1.0)')\n",
                "ax2.legend()\n",
                "ax2.grid(True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('grafico_completo_metrics.png')\n",
                "print(\"Gráfico salvo: grafico_completo_metrics.png\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5a96fe1a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "==============================\n",
                        "GERANDO TEXTO EM PORTUGUÊS\n",
                        "==============================\n",
                        "\n"
                    ]
                },
                {
                    "ename": "NameError",
                    "evalue": "name 'model' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGERANDO TEXTO EM PORTUGUÊS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m30\u001b[39m + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel\u001b[49m.eval()\n",
                        "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
                    ]
                }
            ],
            "source": [
                "# Geração de Texto\n",
                "print(\"\\n\" + \"=\"*30)\n",
                "print(\"GERANDO TEXTO EM PORTUGUÊS\")\n",
                "print(\"=\"*30 + \"\\n\")\n",
                "\n",
                "model.eval()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "24cf13bc",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Gerar texto a partir do vazio\n",
                "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
                "generated_ids = model.generate(context, max_new_tokens=200)\n",
                "\n",
                "# Decode usando tiktoken\n",
                "print(enc.decode(generated_ids[0].tolist()))\n",
                "\n",
                "# Salvando\n",
                "torch.save(model.state_dict(), 'gpt_ptbr_v5.pth')\n",
                "print(\"Modelo salvo como gpt_ptbr_v5.pth\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
